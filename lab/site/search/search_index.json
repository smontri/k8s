{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"Lab_0/","text":"Lab 0: Environment Infrastructure All infrastructure has been deployed on IBM Cloud Connect to the K8s Master We use ssh to connect to the master. ssh k8s-userXX@169.51.31.147 Please use the credentials from the user which has been assigned to you. Check that cluster is ready In order to check that the cluster is running, use following command: kubectl get nodes Output should look like this","title":"Lab 0: Environment"},{"location":"Lab_0/#lab-0-environment","text":"","title":"Lab 0: Environment"},{"location":"Lab_0/#infrastructure","text":"All infrastructure has been deployed on IBM Cloud","title":"Infrastructure"},{"location":"Lab_0/#connect-to-the-k8s-master","text":"We use ssh to connect to the master. ssh k8s-userXX@169.51.31.147 Please use the credentials from the user which has been assigned to you.","title":"Connect to the K8s Master"},{"location":"Lab_0/#check-that-cluster-is-ready","text":"In order to check that the cluster is running, use following command: kubectl get nodes Output should look like this","title":"Check that cluster is ready"},{"location":"Lab_1/","text":"Lab 1: Application deployment Connect to the K8s cluster Use Lab 0 information and credentials to connect to Create a deployment We will deploy a NGINX application using a Kubernetes Deployment. Create the YAML file nginx_deploy.yaml in your home directory with below content: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy the application, use the following command: kubectl apply -f nginx_deploy.yaml -n <your-namespace> --record # -- record allow history of deployment updates Following objects are created: Deployment: kubectl get deploy -n <your-namespace> ReplicaSet: kubectl get rs -n <your-namespace> Pods: kubectl get pods -n <your-namespace> Expose application To expose the application, we will create a service. For this example, we will use ClusterIP/NodePort option. This will allow to call the service from any Node using the NodePort that has been assigned to the service. Create the YAML file nginx_service.yaml in your home directory with below content: apiVersion: v1 kind: Service metadata: name: nginx-nodeport spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 8080 #nodePort: 30080 To expose the application, use the following command: kubectl apply -f nginx_service.yaml -n <your-namespace> Scale application We will now scale the application in order to have 5 replicas for the nginx deployment. kubectl scale --replicas=5 deployment/nginx -n <your-namespace> Have a look at the number of pods afterwards: Additionally, you can show labels applied on Pods: kubectl get pods --show-labels -n <your-namespace> Rollout & Rollback deployment We will now update the deployment to change the Docker image used (from nginx:1.7.9 to nginx:1.9.1) kubectl set image deployment/nginx nginx=nginx:1.9.1 -n <your-namespace> --record Let's see the updated deployment in details kubectl describe deploy/nginx -n demo We see that image has been updated! Run the following command to update the deployment one more time: kubectl set image deployment/nginx nginx=nginx:1.91 -n <your-namespace> --record Now, we check the history of rollouts: kubectl rollout history deployment/nginx -n <your-namespace> Unfortunately we made a mistake with the image name... nginx:1.91 instead of nginx:1.9.1 We will then perform a rollback to revision 2: kubectl rollout undo deployment.v1.apps/nginx -n <your-namespace> --to-revision=2 THIS ENDS LAB #1!","title":"Lab 1: Application deployment"},{"location":"Lab_1/#lab-1-application-deployment","text":"","title":"Lab 1: Application deployment"},{"location":"Lab_1/#connect-to-the-k8s-cluster","text":"Use Lab 0 information and credentials to connect to","title":"Connect to the K8s cluster"},{"location":"Lab_1/#create-a-deployment","text":"We will deploy a NGINX application using a Kubernetes Deployment. Create the YAML file nginx_deploy.yaml in your home directory with below content: apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 To deploy the application, use the following command: kubectl apply -f nginx_deploy.yaml -n <your-namespace> --record # -- record allow history of deployment updates Following objects are created:","title":"Create a deployment"},{"location":"Lab_1/#deployment","text":"kubectl get deploy -n <your-namespace>","title":"Deployment:"},{"location":"Lab_1/#replicaset","text":"kubectl get rs -n <your-namespace>","title":"ReplicaSet:"},{"location":"Lab_1/#pods","text":"kubectl get pods -n <your-namespace>","title":"Pods:"},{"location":"Lab_1/#expose-application","text":"To expose the application, we will create a service. For this example, we will use ClusterIP/NodePort option. This will allow to call the service from any Node using the NodePort that has been assigned to the service. Create the YAML file nginx_service.yaml in your home directory with below content: apiVersion: v1 kind: Service metadata: name: nginx-nodeport spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 80 targetPort: 8080 #nodePort: 30080 To expose the application, use the following command: kubectl apply -f nginx_service.yaml -n <your-namespace>","title":"Expose application"},{"location":"Lab_1/#scale-application","text":"We will now scale the application in order to have 5 replicas for the nginx deployment. kubectl scale --replicas=5 deployment/nginx -n <your-namespace> Have a look at the number of pods afterwards: Additionally, you can show labels applied on Pods: kubectl get pods --show-labels -n <your-namespace>","title":"Scale application"},{"location":"Lab_1/#rollout-rollback-deployment","text":"We will now update the deployment to change the Docker image used (from nginx:1.7.9 to nginx:1.9.1) kubectl set image deployment/nginx nginx=nginx:1.9.1 -n <your-namespace> --record Let's see the updated deployment in details kubectl describe deploy/nginx -n demo We see that image has been updated! Run the following command to update the deployment one more time: kubectl set image deployment/nginx nginx=nginx:1.91 -n <your-namespace> --record Now, we check the history of rollouts: kubectl rollout history deployment/nginx -n <your-namespace> Unfortunately we made a mistake with the image name... nginx:1.91 instead of nginx:1.9.1 We will then perform a rollback to revision 2: kubectl rollout undo deployment.v1.apps/nginx -n <your-namespace> --to-revision=2 THIS ENDS LAB #1!","title":"Rollout &amp; Rollback deployment"},{"location":"Lab_2/","text":"Lab 2: Deploy PHP Guestbook application with Redis Start up the Redis master The guestbook application uses Redis to store its data. It writes its data to a Redis master instance and reads data from multiple Redis slave instances. Creating the Redis Master Deployment The manifest file, included below, specifies a Deployment controller that runs a single replica Redis master Pod. apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: k8s.gcr.io/redis:e2e # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Apply the Redis Master Deployment from the redis-master-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml -n <your-namespace> Query the list of Pods to verify that the Redis Master Pod is running: kubectl get pods -n <your-namespace> Creating the Redis Master Service The guestbook applications needs to communicate to the Redis master to write its data. You need to apply a Service to proxy the traffic to the Redis master Pod. A Service defines a policy to access the Pods. apiVersion: v1 kind: Service metadata: name: redis-master labels: app: redis role: master tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend Apply the Redis Master Service from the redis-master-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml -n <your-namespace> You can check the logs of the Redis Master pod with the command: kubectl logs -f <POD-NAME> Start up the Redis Slaves Creating the Redis Slave Deployment Deployments scale based off of the configurations set in the manifest file. In this case, the Deployment object specifies two replicas. If there are not any replicas running, this Deployment would start the two replicas on your container cluster. Conversely, if there are more than two replicas are running, it would scale down until two replicas are running. apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-slave labels: app: redis spec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: labels: app: redis role: slave tier: backend spec: containers: - name: slave image: gcr.io/google_samples/gb-redisslave:v3 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 6379 Apply the Redis Slave Deployment from the redis-slave-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml -n <your-namespace> Creating the Redis Slave Service The guestbook application needs to communicate to Redis slaves to read data. To make the Redis slaves discoverable, you need to set up a Service. A Service provides transparent load balancing to a set of Pods. apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave tier: backend spec: ports: - port: 6379 selector: app: redis role: slave tier: backend Apply the Redis Slave Sercice from the redis-slave-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml -n <your-namespace> Set Up and Expose the Guestbook Frontend The guestbook application has a web frontend serving the HTTP requests written in PHP. It is configured to connect to the redis-master Service for write requests and the redis-slave service for Read requests. Creating the Guestbook Frontend Deployment apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: frontend labels: app: guestbook spec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v4 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 80 Apply the Frontend Deployment from the frontend-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml -n <your-namespace> And query list of Pods to verify that the three frontend replicas are running kubectl get pods -l app=guestbook -l tier=frontend -n <your-namespace> Creating the Frontend Service The redis-slave and redis-master Services you applied are only accessible within the container cluster because the default type for a Service is ClusterIP . ClusterIP provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster. If you want guests to be able to access your guestbook, you must configure the frontend Service to be externally visible, so a client can request the Service from outside the container cluster. We will expose the Frontend Service through NodePort . apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: NodePort # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend Apply the Frontend Service from the frontend-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml -n <your-namespace> Accessing the application Guestbook Exposing an application through NodePort, enables the access to the application from any of the nodes of the cluster by using the port defined in the service. THIS ENDS LAB #2!","title":"Lab 2: Deploy PHP Guestbook application with Redis"},{"location":"Lab_2/#lab-2-deploy-php-guestbook-application-with-redis","text":"","title":"Lab 2: Deploy PHP Guestbook application with Redis"},{"location":"Lab_2/#start-up-the-redis-master","text":"The guestbook application uses Redis to store its data. It writes its data to a Redis master instance and reads data from multiple Redis slave instances.","title":"Start up the Redis master"},{"location":"Lab_2/#creating-the-redis-master-deployment","text":"The manifest file, included below, specifies a Deployment controller that runs a single replica Redis master Pod. apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-master labels: app: redis spec: selector: matchLabels: app: redis role: master tier: backend replicas: 1 template: metadata: labels: app: redis role: master tier: backend spec: containers: - name: master image: k8s.gcr.io/redis:e2e # or just image: redis resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Apply the Redis Master Deployment from the redis-master-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml -n <your-namespace> Query the list of Pods to verify that the Redis Master Pod is running: kubectl get pods -n <your-namespace>","title":"Creating the Redis Master Deployment"},{"location":"Lab_2/#creating-the-redis-master-service","text":"The guestbook applications needs to communicate to the Redis master to write its data. You need to apply a Service to proxy the traffic to the Redis master Pod. A Service defines a policy to access the Pods. apiVersion: v1 kind: Service metadata: name: redis-master labels: app: redis role: master tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: master tier: backend Apply the Redis Master Service from the redis-master-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml -n <your-namespace> You can check the logs of the Redis Master pod with the command: kubectl logs -f <POD-NAME>","title":"Creating the Redis Master Service"},{"location":"Lab_2/#start-up-the-redis-slaves","text":"","title":"Start up the Redis Slaves"},{"location":"Lab_2/#creating-the-redis-slave-deployment","text":"Deployments scale based off of the configurations set in the manifest file. In this case, the Deployment object specifies two replicas. If there are not any replicas running, this Deployment would start the two replicas on your container cluster. Conversely, if there are more than two replicas are running, it would scale down until two replicas are running. apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: redis-slave labels: app: redis spec: selector: matchLabels: app: redis role: slave tier: backend replicas: 2 template: metadata: labels: app: redis role: slave tier: backend spec: containers: - name: slave image: gcr.io/google_samples/gb-redisslave:v3 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 6379 Apply the Redis Slave Deployment from the redis-slave-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml -n <your-namespace>","title":"Creating the Redis Slave Deployment"},{"location":"Lab_2/#creating-the-redis-slave-service","text":"The guestbook application needs to communicate to Redis slaves to read data. To make the Redis slaves discoverable, you need to set up a Service. A Service provides transparent load balancing to a set of Pods. apiVersion: v1 kind: Service metadata: name: redis-slave labels: app: redis role: slave tier: backend spec: ports: - port: 6379 selector: app: redis role: slave tier: backend Apply the Redis Slave Sercice from the redis-slave-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml -n <your-namespace>","title":"Creating the Redis Slave Service"},{"location":"Lab_2/#set-up-and-expose-the-guestbook-frontend","text":"The guestbook application has a web frontend serving the HTTP requests written in PHP. It is configured to connect to the redis-master Service for write requests and the redis-slave service for Read requests.","title":"Set Up and Expose the Guestbook Frontend"},{"location":"Lab_2/#creating-the-guestbook-frontend-deployment","text":"apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2 kind: Deployment metadata: name: frontend labels: app: guestbook spec: selector: matchLabels: app: guestbook tier: frontend replicas: 3 template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google-samples/gb-frontend:v4 resources: requests: cpu: 100m memory: 100Mi env: - name: GET_HOSTS_FROM value: dns # Using `GET_HOSTS_FROM=dns` requires your cluster to # provide a dns service. As of Kubernetes 1.3, DNS is a built-in # service launched automatically. However, if the cluster you are using # does not have a built-in DNS service, you can instead # access an environment variable to find the master # service's host. To do so, comment out the 'value: dns' line above, and # uncomment the line below: # value: env ports: - containerPort: 80 Apply the Frontend Deployment from the frontend-deployment.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml -n <your-namespace> And query list of Pods to verify that the three frontend replicas are running kubectl get pods -l app=guestbook -l tier=frontend -n <your-namespace>","title":"Creating the Guestbook Frontend Deployment"},{"location":"Lab_2/#creating-the-frontend-service","text":"The redis-slave and redis-master Services you applied are only accessible within the container cluster because the default type for a Service is ClusterIP . ClusterIP provides a single IP address for the set of Pods the Service is pointing to. This IP address is accessible only within the cluster. If you want guests to be able to access your guestbook, you must configure the frontend Service to be externally visible, so a client can request the Service from outside the container cluster. We will expose the Frontend Service through NodePort . apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # comment or delete the following line if you want to use a LoadBalancer type: NodePort # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer ports: - port: 80 selector: app: guestbook tier: frontend Apply the Frontend Service from the frontend-service.yaml file: kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml -n <your-namespace>","title":"Creating the Frontend Service"},{"location":"Lab_2/#accessing-the-application-guestbook","text":"Exposing an application through NodePort, enables the access to the application from any of the nodes of the cluster by using the port defined in the service. THIS ENDS LAB #2!","title":"Accessing the application Guestbook"}]}